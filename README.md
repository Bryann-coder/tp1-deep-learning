# Projet : Cycle de Vie d'un Mod√®le de Reconnaissance de Chiffres (MNIST)

Ce projet illustre le cycle de vie complet d'un mod√®le de Deep Learning, de l'exp√©rimentation √† la production. Il comprend l'entra√Ænement d'un classificateur de chiffres sur le jeu de donn√©es MNIST, le suivi des exp√©riences avec MLflow, l'encapsulation du meilleur mod√®le dans une API web Flask, et son d√©ploiement via Docker.

## Architecture du Projet

Le projet est structur√© pour s√©parer clairement les diff√©rentes √©tapes du cycle de vie du mod√®le : entra√Ænement, service (API) et test.

```
/
|-- mlruns/                     # (G√©n√©r√©) Dossier de MLflow pour le suivi des exp√©riences
|-- mnist_model.h5              # Mod√®le entra√Æn√©, pr√™t √† √™tre utilis√© par l'API
|-- train_model.py              # Script pour entra√Æner et comparer les mod√®les
|-- app.py                      # Application Flask pour servir le mod√®le via une API REST
|-- test.py                     # Script client pour tester l'API
|-- Dockerfile                  # Instructions pour construire l'image Docker de l'API
|-- requirements.txt            # D√©pendances Python du projet
|-- README.md                   # Ce fichier
`-- .gitignore                  # Fichiers et dossiers √† ignorer par Git
```

### R√¥le des Fichiers

| Fichier                 | Description                                                                                                                                                                                                |
| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **`train_model.py`**    | **Script d'entra√Ænement** : Charge les donn√©es MNIST, d√©finit une architecture de r√©seau de neurones, et entra√Æne plusieurs versions du mod√®le en comparant diff√©rents optimiseurs (`Adam`, `SGD`, `RMSprop`). Utilise **MLflow** pour enregistrer les param√®tres, les m√©triques et les mod√®les de chaque exp√©rimentation. |
| **`app.py`**            | **Application API** : Utilise le framework **Flask** pour cr√©er un serveur web. Il charge le fichier `mnist_model.h5` et expose un endpoint `/predict` qui accepte des donn√©es d'image en JSON et retourne une pr√©diction. |
| **`test.py`**           | **Script de test client** : Simule un client qui interroge l'API. Il charge une image locale (`mon_chiffre.png`), la pr√©-traite pour qu'elle corresponde au format attendu par le mod√®le, envoie une requ√™te POST √† l'API Dockeris√©e et affiche la pr√©diction de mani√®re lisible. |
| **`Dockerfile`**        | **Fichier de conteneurisation** : Contient les instructions pour construire une image **Docker**. Cette image embarque Python, les d√©pendances n√©cessaires, le code de l'API (`app.py`) et le mod√®le (`mnist_model.h5`) pour cr√©er un conteneur portable et isol√©. |
| **`requirements.txt`**  | **D√©pendances** : Liste toutes les biblioth√®ques Python n√©cessaires (`tensorflow`, `flask`, `mlflow`, `numpy`, `requests`, `Pillow`) pour que le projet fonctionne. |
| **`mnist_model.h5`**    | **Le mod√®le** : Fichier binaire contenant l'architecture et les poids du r√©seau de neurones entra√Æn√©. **Note importante** : ce fichier n'est pas directement g√©n√©r√©. Il doit √™tre r√©cup√©r√© manuellement depuis les "artefacts" de la meilleure ex√©cution dans l'interface de MLflow. |
| **`mlruns/`**           | **Suivi MLflow** : R√©pertoire cr√©√© et g√©r√© automatiquement par MLflow. Il contient toutes les informations sur les ex√©cutions (param√®tres, m√©triques, artefacts comme les mod√®les) de mani√®re structur√©e. |

---

## Guide d'Utilisation Complet

Suivez ces √©tapes pour ex√©cuter le projet du d√©but √† la fin.

### √âtape 1 : Pr√©requis

Assurez-vous d'avoir install√© les outils suivants sur votre machine :
-   Python 3.8+ et `pip`
-   Git
-   Docker Desktop (et assurez-vous qu'il est en cours d'ex√©cution)

### √âtape 2 : Installation

1.  **Clonez le d√©p√¥t et naviguez dans le r√©pertoire :**
    ```bash
    git clone <URL_de_votre_d√©p√¥t>
    cd <nom_du_d√©p√¥t>
    ```

2.  **Cr√©ez un environnement virtuel et activez-le :**
    ```bash
    python -m venv venv
    # Sur Windows
    venv\Scripts\activate
    # Sur macOS/Linux
    source venv/bin/activate
    ```

3.  **Cr√©ez le fichier `requirements.txt`** avec le contenu suivant :
    ```txt
    tensorflow
    flask
    numpy
    mlflow
    requests
    Pillow
    ```

4.  **Installez les d√©pendances :**
    ```bash
    pip install -r requirements.txt
    ```

### √âtape 3 : Entra√Ænement et S√©lection du Mod√®le

1.  **Lancez le script d'entra√Ænement :**
    Ce script va entra√Æner trois mod√®les diff√©rents et enregistrer les r√©sultats dans MLflow.
    ```bash
    python train_model.py
    ```

2.  **Visualisez les r√©sultats avec MLflow :**
    Lancez l'interface utilisateur de MLflow pour comparer les performances des mod√®les.
    ```bash
    mlflow ui
    ```
    Ouvrez votre navigateur √† l'adresse [http://127.0.0.1:5000](http://127.0.0.1:5000).

3.  **Exportez le meilleur mod√®le :**
    a. Dans l'interface MLflow, trouvez l'ex√©cution (`run`) avec la meilleure `final_test_accuracy`.
    b. Cliquez sur cette ex√©cution pour voir ses d√©tails.
    c. Dans la section **"Artifacts"**, vous verrez un dossier (`model_Adam`, par exemple). Cliquez dessus.
    d. Vous y trouverez le fichier `model.h5`. **T√©l√©chargez-le, placez-le √† la racine de votre projet et renommez-le `mnist_model.h5`**.

    Votre projet doit maintenant contenir le fichier `mnist_model.h5`.

### √âtape 4 : D√©ploiement de l'API avec Docker

Maintenant que nous avons le mod√®le, nous pouvons construire l'image Docker de notre API.

1.  **Construisez l'image Docker :**
    Cette commande lit le `Dockerfile` et assemble une image nomm√©e `mnist-api`.
    ```bash
    docker build -t mnist-api .
    ```

2.  **Lancez le conteneur Docker :**
    Cette commande d√©marre un conteneur √† partir de l'image, en mappant le port 5000 du conteneur au port 5000 de votre machine.
    ```bash
    docker run -p 5000:5000 mnist-api
    ```
    L'API est maintenant en cours d'ex√©cution et accessible √† `http://localhost:5000`.

### √âtape 5 : Tester l'API de Pr√©diction

Pour tester l'API, nous allons utiliser le script `test.py`, qui n√©cessite une image de chiffre.

1.  **Cr√©ez une image de test :**
    -   Ouvrez un logiciel de dessin simple (comme Paint, GIMP, etc.).
    -   Cr√©ez une petite image (ex: 100x100 pixels) avec un **fond noir**.
    -   Dessinez un chiffre **en blanc** au centre.
    -   Enregistrez l'image sous le nom `mon_chiffre.png` √† la racine de votre projet.

2.  **Ex√©cutez le script de test :**
    Assurez-vous que votre conteneur Docker est toujours en cours d'ex√©cution, puis lancez :
    ```bash
    python test.py
    ```

3.  **Analysez le r√©sultat :**
    Le script affichera la pr√©diction du mod√®le ainsi que les 10 pr√©dictions les plus probables avec leur score de confiance.

    **Exemple de sortie attendue :**
    ```
    Pr√©paration de l'image 'mon_chiffre.png'...
    üöÄ Envoi de la requ√™te √† l'API...

    ‚úÖ Pr√©diction re√ßue !
       Le mod√®le pense que ce chiffre est un : 7

    üìä Top des pr√©dictions les plus probables :
       1. Chiffre 7 (Confiance : 99.85%)
       2. Chiffre 2 (Confiance : 0.11%)
       3. Chiffre 1 (Confiance : 0.02%)
       ...
    ```

F√©licitations ! Vous avez compl√©t√© tout le cycle : entra√Ænement, s√©lection, d√©ploiement et test d'un mod√®le de Deep Learning.